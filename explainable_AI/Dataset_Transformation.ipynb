{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affineTrans(image_path):\n",
    "# Load an image\n",
    "  image = Image.open(image_path)\n",
    "\n",
    "# Define the RandomAffine transformation\n",
    "  random_affine = transforms.RandomAffine(degrees=30, translate=(0.5, 0.5), scale=(0.8, 1.2), shear=10)\n",
    "\n",
    "# Apply the transformation to the image\n",
    "  transformed_image = random_affine(image)\n",
    "\n",
    "# Display the original and transformed images\n",
    "#image.show()\n",
    "  return transformed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_uniform(min_val, max_val):\n",
    "    random_value = random.uniform(0, 1)\n",
    "    scaled_value = random_value * (max_val - min_val) + min_val\n",
    "\n",
    "    # Cap the value at the maximum if it exceeds it\n",
    "    if scaled_value > max_val:\n",
    "        scaled_value = max_val\n",
    "\n",
    "    # Truncate to one digit after the decimal point\n",
    "    scaled_value = round(scaled_value, 1)\n",
    "\n",
    "    return scaled_value\n",
    "\n",
    "# Example usage\n",
    "uniform_variable = generate_uniform(0.4, 0.8)\n",
    "print(uniform_variable)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def snipping(image_path):\n",
    "   image = cv2.imread(image_path)\n",
    "\n",
    "   # Convert the image to grayscale\n",
    "   gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "   # Apply thresholding to create a binary image\n",
    "   ret, thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)\n",
    "\n",
    "  # Find contours in the binary image\n",
    "   contours, hierarchy = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "   # Draw contours on the original image\n",
    "   #cv2.drawContours(image, contours, -1, (0, 255, 0), 3)\n",
    "   cnt = contours[0]\n",
    "\n",
    "   # compute the bounding rectangle of the contour\n",
    "   x,y,w,h = cv2.boundingRect(cnt)\n",
    "\n",
    "   # draw contour\n",
    "   #img = cv2.drawContours(image,[cnt],0,(0,255,255),2)\n",
    "   x=x-5\n",
    "   y=y-5\n",
    "   w=w+10\n",
    "   h=h+10\n",
    "   # draw the bounding rectangle\n",
    "   img = cv2.rectangle(image,(x,y),(x+w,y+h),(0,0,0),2)\n",
    "   cv2.waitKey(0)\n",
    "   cv2.destroyAllWindows()\n",
    "   box_image = img[y : y+h, x: x+w]\n",
    "   return box_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output(img1):\n",
    "  img1 = Image.fromarray(cv2.cvtColor(img1, cv2.COLOR_BGR2RGB))\n",
    "  new_image = Image.new(\"RGB\", (600,600))\n",
    "  new_image.paste(img1, (250,250))\n",
    "  final=cv2.cvtColor(np.array(new_image), cv2.COLOR_RGB2BGR)\n",
    "  final = cv2.resize(final, (224, 224))\n",
    "  return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/home/stud1/Desktop/VGG16_Train/Data Sample/Arborio'\n",
    "output_dir = '/home/stud1/Desktop/VGG16_Train/Transformed_dataset/Arborio'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "#/home/stud1/Desktop/VGG16_Train/Transformed_dataset/Arborio\n",
    "# Iterate over the images in the input directory\n",
    "i=1\n",
    "while(i<=100):\n",
    "    test=snipping()\n",
    "    test1=affineTrans(\"/home/stud1/Desktop/VGG16_Train/Data Sample/Arborio/Arborio (\"+str(i)+\").jpg\")\n",
    "    output_path = os.path.join(output_dir, \"arborio (\"+str(i)+\").jpg\")\n",
    "    test1.save(output_path)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/home/stud1/Desktop/VGG16_Train/Data Sample/Basmati'\n",
    "output_dir = '/home/stud1/Desktop/VGG16_Train/Transformed_dataset/Basmati'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "#/home/stud1/Desktop/VGG16_Train/Transformed_dataset/Arborio\n",
    "# Iterate over the images in the input directory\n",
    "i=1\n",
    "while(i<=100):\n",
    "    test1=affineTrans(\"/home/stud1/Desktop/VGG16_Train/Data Sample/Basmati/basmati (\"+str(i)+\").jpg\")\n",
    "    output_path = os.path.join(output_dir, \"basmati (\"+str(i)+\").jpg\")\n",
    "    test1.save(output_path)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/home/stud1/Desktop/VGG16_Train/Data Sample/Ipsala'\n",
    "output_dir = '/home/stud1/Desktop/VGG16_Train/Transformed_dataset/Ipsala'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "#/home/stud1/Desktop/VGG16_Train/Transformed_dataset/Arborio\n",
    "# Iterate over the images in the input directory\n",
    "i=1\n",
    "while(i<=100):\n",
    "    test1=affineTrans(\"/home/stud1/Desktop/VGG16_Train/Data Sample/Ipsala/Ipsala (\"+str(i)+\").jpg\")\n",
    "    output_path = os.path.join(output_dir, \"ipsala (\"+str(i)+\").jpg\")\n",
    "    test1.save(output_path)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/home/stud1/Desktop/VGG16_Train/Data Sample/Jasmine'\n",
    "output_dir = '/home/stud1/Desktop/VGG16_Train/Transformed_dataset/Jasmine'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "#/home/stud1/Desktop/VGG16_Train/Transformed_dataset/Arborio\n",
    "# Iterate over the images in the input directory\n",
    "i=1\n",
    "while(i<=100):\n",
    "    test1=affineTrans(\"/home/stud1/Desktop/VGG16_Train/Data Sample/Jasmine/Jasmine (\"+str(i)+\").jpg\")\n",
    "    output_path = os.path.join(output_dir, \"jasmine (\"+str(i)+\").jpg\")\n",
    "    test1.save(output_path)\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/home/stud1/Desktop/VGG16_Train/Data Sample/Karacadag'\n",
    "output_dir = '/home/stud1/Desktop/VGG16_Train/Transformed_dataset/Karacadag'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "#/home/stud1/Desktop/VGG16_Train/Transformed_dataset/Arborio\n",
    "# Iterate over the images in the input directory\n",
    "i=1\n",
    "while(i<=100):\n",
    "    test1=affineTrans(\"/home/stud1/Desktop/VGG16_Train/Data Sample/Karacadag/Karacadag (\"+str(i)+\").jpg\")\n",
    "    output_path = os.path.join(output_dir, \"karacadag (\"+str(i)+\").jpg\")\n",
    "    test1.save(output_path)\n",
    "    i=i+1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aniket_intern",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
